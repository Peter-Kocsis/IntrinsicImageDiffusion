ddim_config:
  S: 100
  eta: 0

unet_config:
  target: iid.ldm.diffusionmodule.IIDUNetModel
  params:
    image_size: 32  # legacy
    in_channels: 11
    out_channels: 8
    model_channels: 320
    attention_resolutions:
    - 4
    - 2
    - 1
    num_res_blocks: 2
    channel_mult:
    - 1
    - 2
    - 4
    - 4
    num_head_channels: 64
    use_checkpoint: true
    use_spatial_transformer: true
    use_linear_in_transformer: true
    transformer_depth: 1
    context_dim: 1024
    legacy: false

diffusion_config:
  image_size: [ 60, 80 ]
  first_stage_config:
    target: ldm.models.autoencoder.AutoencoderKL
    params:
      ckpt_path: null
      embed_dim: 4
      monitor: val/rec_loss
      ddconfig:
        double_z: true
        z_channels: 4
        resolution: 256
        in_channels: 3
        out_ch: 3
        ch: 128
        ch_mult:
        - 1
        - 2
        - 4
        - 4
        num_res_blocks: 2
        attn_resolutions: []
        dropout: 0.0
      lossconfig:
        target: torch.nn.Identity
  linear_start: 0.00085
  linear_end: 0.012
  num_timesteps_cond: 1
  log_every_t: 1
  timesteps: 1000
  first_stage_key:
  - albedo
  - material
  cond_stage_key: im
  channels: 8
  scale_factor: 0.18215
  use_ema: false
  concat_keys:
  - im
  cond_stage_trainable: false
  conditioning_key: hybrid
  cond_stage_config:
    target: iid.ldm.encoders.FrozenOpenCLIPImageEmbedder
    params:
      freeze: true
      layer: pooled
  finetune_keys: null
  concat_encoding_stage_config:
    target: ldm.modules.diffusionmodules.model.Encoder
    params:
      double_z: false
      z_channels: 3
      resolution: 256
      in_channels: 3
      ch: 128
      out_ch: 1
      ch_mult:
      - 1
      - 2
      - 4
      - 4
      num_res_blocks: 2
      attn_resolutions: []
      dropout: 0.0
