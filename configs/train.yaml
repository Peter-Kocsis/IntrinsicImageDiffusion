# @package _global_

# to execute this experiment run:
# python train.py iid.train

defaults:
  - /environment@_here_: default
  - /logger: wandb
  - /data: interiorverse

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

task_name: train

seed: 0

data:
  sampling_cfg:
    batch_size: 10
    num_workers: 10
  dataset_cfg:
    features_to_include: [ "im", "albedo", "material" ]

model:
  _target_: iid.material_diffusion.iid.IntrinsicImageDiffusion
  unet_config:
    target: iid.material_diffusion.ldm.diffusionmodule.IIDUNetModel
    params:
      image_size: 32  # legacy
      in_channels: 11
      out_channels: 8
      model_channels: 320
      attention_resolutions:
        - 4
        - 2
        - 1
      num_res_blocks: 2
      channel_mult:
        - 1
        - 2
        - 4
        - 4
      num_head_channels: 64
      use_checkpoint: true
      use_spatial_transformer: true
      use_linear_in_transformer: true
      transformer_depth: 1
      context_dim: 1024
      legacy: false
  diffusion_config:
    image_size: 32
    ckpt_path: models/stable_diffusion/512-depth-ema.ckpt
    first_stage_config:
      target: ldm.models.autoencoder.AutoencoderKL
      params:
        ckpt_path: null
        embed_dim: 4
        monitor: val/rec_loss
        ddconfig:
          double_z: true
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
            - 1
            - 2
            - 4
            - 4
          num_res_blocks: 2
          attn_resolutions: [ ]
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity
    linear_start: 0.00085
    linear_end: 0.012
    num_timesteps_cond: 1
    log_every_t: 1
    timesteps: 1000
    first_stage_key:
      - albedo
      - material
    cond_stage_key: im
    channels: 8
    scale_factor: 0.18215
    use_ema: false
    concat_keys:
      - im
    cond_stage_trainable: false
    conditioning_key: hybrid
    cond_stage_config:
      target: iid.material_diffusion.ldm.encoders.FrozenOpenCLIPImageEmbedder
      params:
        freeze: true
        layer: pooled
    finetune_keys: null
    concat_encoding_stage_config:
      target: ldm.modules.diffusionmodules.model.Encoder
      params:
        double_z: false
        z_channels: 3
        resolution: 256
        in_channels: 3
        ch: 128
        out_ch: 1
        ch_mult:
          - 1
          - 2
          - 4
          - 4
        num_res_blocks: 2
        attn_resolutions: [ ]
        dropout: 0.0
  learning_rate: 1e-5


trainer:
  _target_: pytorch_lightning.Trainer
  min_epochs: 250
  max_epochs: 250
  accelerator: gpu
  devices: 4
  sync_batchnorm: True
  strategy:
    _target_: pytorch_lightning.strategies.ddp.DDPStrategy
    find_unused_parameters: False
  default_root_dir: ${paths.output_dir}
  deterministic: false  # XFormers requires it to be false
  check_val_every_n_epoch: 9999
  log_every_n_steps: 1
  fast_dev_run: False
  num_sanity_val_steps: 0

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    filename: null
    monitor: "loss/train_loss"
    verbose: False
    save_top_k: 1
    mode: "min"
    save_weights_only: False
    every_n_train_steps: null
    train_time_interval: null
    every_n_epochs: null
    dirpath: ${paths.output_dir}/checkpoints
    save_last: True
    save_on_train_epoch_end: True
    auto_insert_metric_name: False
  loss_logger:
    _target_: iid.callbacks.BatchLogger
    context: loss
    output_keys_to_log: [ "loss*" ]
    batch_keys_to_log: [ ]
    is_metric: True
    log_schedule:
      on_train_batch_end: ::1
  diffusion_sampler:
    _target_: iid.callbacks.DiffusionSampler
    sanple_id: 0
    n_samples: 4
    log_schedule:
      on_train_epoch_start: ::1